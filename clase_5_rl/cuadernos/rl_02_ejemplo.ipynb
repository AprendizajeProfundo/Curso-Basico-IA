{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a09c1e-1e42-44d8-92b7-7d07206ea950",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ccee70-a067-4185-910d-626c44b0862f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Minicurso  de Inteligencia Artificial<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc290f0c-2d66-4d24-9b46-99bab8a8dc2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:Green\"><center>Ejmplo de aprendizaje reforzado. Entropía cruzada</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b128e-6c3b-4b0a-b415-98b3b2405de7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65844f96-a7af-4eab-a5b7-4a6a5e2c3f2b",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "2. Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "1. Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80320c63-e11d-4629-ad18-d1c053503a22",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4bc84-de76-4fd2-af79-bca8ea26c6c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088caa7-c36a-44fd-b556-f283153a7acf",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3a27d-c537-4f52-b770-43d35f3de5a2",
   "metadata": {},
   "source": [
    "1. Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "1. Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f14fdf-6c47-490d-b982-3e95b8dfef03",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69f125-2851-448c-8496-0de0b1886f0c",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [La API OpenAI Gym](#La-API-OpenAI-Gym)\n",
    "* [El ambiente CartPole](#El-ambiente-CartPole)\n",
    "* [El método de entropía cruzada en CartPole](#El-método-de-entropía-cruzada-en-CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "En esta lección vamos a entrenar un agente con base en una red neuronal, la cual será una red de clasificación con función de pérdida entropía cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b92992-3c1b-4af4-b8b0-44ac0460c5b4",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">La API OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa5bf5-614f-4420-a33e-81936045c7a0",
   "metadata": {},
   "source": [
    "La API `OpenAI Gym` tiene una rica colección de ambientes para experimentos de aprendizaje reforzado (AR), usando una interfaz unificada.\n",
    "\n",
    "La clase principal de Gym es *Env* (de environment). Sus métodos y atributos proveen la información necesaria para poder entrenar agentes que interactúen con el medio ambientes. Las piezas más importante disponibles con *Env* son:\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../imagenes/environment.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro\n",
    "\n",
    "- Un conjunto de `acciones` que se permite sean ejecutadas en el ambiente.\n",
    "- El tamaño y bordes de las `observaciones` que el ambiente le provee al agente.\n",
    "- Un método *step* para ejecutar un acción. El método regresa la nueva observación, la `recompensa` y la indicación de si el `episodio` ha terminado (*done*).\n",
    "- Un método *reset* que retorna al ambiente a su *estado inicial* y entrega la primera observación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e5fb-ca34-4653-9f3e-c709d652dd81",
   "metadata": {},
   "source": [
    "### El espacio de acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157683-e2d3-43ff-8f29-7bd14309fffe",
   "metadata": {},
   "source": [
    "Las acciones que puede realizar un agente puede ser discretas, continuas o incluso una combinación de ambas. Por ejemplo en un automóvil manejado de manera autónoma puede ser posible oprimir varios botones a la vez (discreto), oprimir el pedal del acelerador o del freno (continuo) y así. El espacio de acciones contiene o modela todas las posibilidades de acción del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ad16b-e42c-4957-8524-5476670451bd",
   "metadata": {},
   "source": [
    "### El espacio de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1305-62f4-4dd5-91a4-fe68ce77191e",
   "metadata": {},
   "source": [
    "Las observaciones son piezas de información que el ambiente puede entregar al agente en cualquier momento del tiempo, además de la recompensa. Las observaciones pueden ser muy simples como un arreglo unidimensional de números o  complejos tensores, como las imágenes provenientes de varias cámaras del automóvil.\n",
    "\n",
    "Es claro que acciones y observaciones tienen similaridades, por lo que Gym dispone de una clase abstracta llamada *Space* que permite derivar espacios de acciones y espacios de observaciones. La siguiente imagen muestra una diagrama de la clase *Space*.\n",
    "\n",
    "Existen otras subclases de *Space*, pero estas son las más comúnmente utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374739ea-141b-4a29-89e1-2ef87bed6b80",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../imagenes/jerarquia_clase_Space_Gym.jpeg\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Jerarquía de la clase Space de OpenAI Gym. Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc147e-8c2f-4028-ad45-aa897b0fd0fb",
   "metadata": {},
   "source": [
    "La clase abstracta *Space* incluye dos métodos relevantes:\n",
    "\n",
    "* *sample()*: retorna una muestra aleatoria del espacio.\n",
    "* *contains(x)*: chequea si el argumento es un elemento del dominio del espacio.\n",
    "\n",
    "Ambos métodos son abstractos y deben ser implementados en las subclases de *Space*.\n",
    "\n",
    "* La clase *Discrete* representa un conjunto de ítems mutuamente excluyentes, numerados entre 0 y $n-1$. Su único atributo *n*, es una cuenta de los ítems que la clase describe. Por ejemplo. *Discrete(n=4)* puede representar por ejemplo un espacio de acciones de cuatro direcciones para moverse: *[left, right, up, down]*.\n",
    "\n",
    "* La clase *Box* representa un tensor *n*-dimensional de números racionales con intervalos *[low, high]*. Por ejemplo puede representar el pedal de un acelerador con un valor simple entre 0.0 y 1.0. En pedal sería codificado en tal caso como *Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)*. Otro ejemplo puede ser la observación de la pantalla en un juego Atari, la cual es RGB de tamaño 210*160: *Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)*.\n",
    "\n",
    "* La última subclase de *Space* es *Tuple*, la cual permite combinar varias instancias de *Space* juntas. esta subclase permite crear espacios de acciones y observaciones de la complejidad que se desee.\n",
    "\n",
    "Un ejemplo del uso de tupla puede ser el siguiente. Supongamos que deseamos modelar algunos controles de un automóvil. Para empezar la dirección (el ángulo de la dirección, en realidad), el acelerador y el freno puede cambiar en cada instante de tiempo. Por otro lado, podemos tener botones discretos para representar la luces direccionales *(apagadas, izquierda, derecha)* y la bocina *(encendida, apagada)*. Entonces, podemos crear *Tupla(spaces=(Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(3), Discrete(2)\n",
    ")*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b93b-c965-4151-9465-f48ffc6d15e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### El ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234d15-59fb-48f8-b091-8a19fc0bfc01",
   "metadata": {},
   "source": [
    "El ambiente es representado por la clase *Env*. Todo ambiente tiene dos miembros de tipo *Space*: \n",
    "\n",
    "* *action_space*: acciones permitidas que entrega el ambiente al agente.\n",
    "* *observation_space*: observaciones proveídas por el ambiente al agente.\n",
    "\n",
    "Además el ambiente tiene los siguientes métodos:\n",
    "\n",
    "* *reset()*: coloca el ambiente en el estado inicial.\n",
    "* *step()*: Permite al agente tomar una acción y retorna información como respuesta a la acción: la siguiente observación, la recompensa y la bandera indicando si el episodio ha  finalizado. Esta es la pieza central de la funcionalidad del ambiente.\n",
    "\n",
    "Estas características permiten crear código genérico que podría trabajar con cualquier ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fea79-4bca-4a0e-800c-38a8c40dc71b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El ambiente CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c5078-0287-4a92-af21-9fd0d6cca4a2",
   "metadata": {},
   "source": [
    "Fuente [OpenAI Gym](https://gym.openai.com/)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../imagenes/Ejemplo_polea.gif\" width=\"200\" height=\"200\" align=\"left\"/>\n",
    "</center>\n",
    "</figure>\n",
    "    \n",
    "Este es nuestro primer acercamiento a OpenAI Gym. En este primer  ejemplo conocido como CartPole es un clásico en la teoría de control. En la animación *gif* a la izquierda  observa el objeto de interés. Es una palo, sostenido en la parte inferior por un pasador unido a una base. Ignoramos posibles complicaciones como la fricción. Es decir suponemos que el palo se mueve libremente sobre el pasador. \n",
    "\n",
    "\n",
    "Obviamente el palo puede caer muy fácilmente, por lo que el propósito es mover la base hacia la izquierda o hacia la derecha para evitar que el palo caiga. Ese es el problema a resolver. El problema puede ser abordado como un problema de control que lleva a diseñan un modelo en ecuaciones diferenciales. Pero en aprendizaje reforzado buscamos resolver el problema, sin utilizar los conceptos de la física correspondientes. De momento vamos a explorar un poco el  ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "La observación que entrega el ambiente es un arreglo de cuatro posiciones conteniendo respectivamente  *coordenada del centro de masas*, *velocidad*, *ángulo con respecto a la plataforma*, y *velocidad angular*.\n",
    "\n",
    "El espacio de acciones es de tipo *Discrete* y contiene dos elementos 0 y 1, o *izquierda* y *derecha*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e0164-7f64-4ddc-ab6d-dc02d10348cd",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El método de entropía cruzada en CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfb14a-2eda-4bb7-8215-77acbaa0ea52",
   "metadata": {},
   "source": [
    "El núcleo de nuestro modelo es una red neuronal(RN) con una capa oculta de de 128 neuronas con un rectificador lineal (ReLU) como función de activación. El tamaño 128 es completamente arbitrario en este ejemplo. Sin embargo nuestro método es muy robusto y converge rápidamente.\n",
    "\n",
    "Comenzamos definiendo algunas constantes globales:\n",
    "\n",
    "- HIDDEN_SIZE: tamaño de la capa oculta.\n",
    "- BATCH SIZE: número de episodios en cada iteración.\n",
    "- PERCENTILE: porcentaje de recompensas totales que vamos a conservar para determinar episodios *elite*. \n",
    "\n",
    "Es decir, vamos a dejar por fuera del entrenamiento de la red, los datos asociados a episodios en donde la recompensa total esté por debajo del 30\\% inferior.\n",
    "\n",
    "Adicionalmente definimos la red neuronal que usaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cb2a9-d77a-4009-8247-0078bec53bd4",
   "metadata": {},
   "source": [
    "### Información de los episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfb0e9-31c4-43be-b560-a039aef307a5",
   "metadata": {},
   "source": [
    "Vamos a definir dos clases auxiliares para manejar la información de los episodios. Estas son clases tupla de tipo `namedtuple`. Este tipo de tupla definida en el módulo estándard *collections* de Python 3+. Es una tupla en la cual cada elemento recibe un nombre, por el cual puede ser llamado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c630da5-15c1-482a-9126-63f90223b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clases namedtuple para manejo de información\n",
    "# de episodios\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d962ff-93b7-4783-be0a-a6dc208cc3b9",
   "metadata": {},
   "source": [
    "*EpisodeStep* se usará para representar la información de un paso simple que el agente hace en un episodio. Tal información es la observación recibida y la acción ejecutado por el agente.\n",
    "\n",
    "*Episode* mantiene la información global de un episodio: la recompensa total recibida en el episodio y la colección completa de EpisodeStep de tal episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf811ec-d716-494e-859d-72e0c9544376",
   "metadata": {},
   "source": [
    "### Como sucede el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b71d3-f58d-4e15-9df6-fff715226f76",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../imagenes/Entropia_cruzada.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ecec7-7a10-4767-9163-386c7ea2ddec",
   "metadata": {},
   "source": [
    "El entrenamiento sucederá de la siguiente forma.\n",
    "\n",
    "1. Se selecciona el 70% de los episodios con más alta recompensa. Estos episodios los llamamos *élite*.\n",
    "1. De los episodios *élite* se extrae la información de observaciones para la entrada de la red y acción para la salida de la red. La red es de clasificación, por lo que se usará como función de pérdida la entropía cruzada, la cual se calcula directamente de los valores de la salida de la red que denominamos en este caso `logits`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d8adc-f826-4e30-bed2-144c7936bb32",
   "metadata": {},
   "source": [
    "#### Objetivo del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd88003-12af-4c26-968a-f180a82aaef1",
   "metadata": {},
   "source": [
    "Buscamos entrenar la red neuronal, con la información de las observaciones, para que prediga la distribución de probabilidades de la siguiente acción. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff804985-4986-4cc9-b338-0259d49f3af8",
   "metadata": {},
   "source": [
    "#### Que hace el agente en cada iteración de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab9062-1d47-4147-879a-e617274ba623",
   "metadata": {},
   "source": [
    "1. El agente en cada paso recibe del ambiente la recompensa, la nueva observación y las posibles acciones disponibles, que este caso siempre son dos: izquierda o derecha.\n",
    "1. Entonces le entrega a la red la nueva observación y recibe como respuesta de un vector de probabilidades para la siguiente acción.\n",
    "3. El agente selecciona la siguiente acción de forma aleatoria, usando el modelo de probabilidad recibido.\n",
    "4. El agente informa al ambiente sobre su siguiente acción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ed70e-2d54-47e0-a7fd-3d8fcdaf2a9e",
   "metadata": {},
   "source": [
    "#### Nota sobre eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a8a6e-8203-49f5-ab00-6ab328f0178e",
   "metadata": {},
   "source": [
    "Es más eficiente usar la función de pérdida de entropía cruzada de Pytorch basada en los logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9bb92-ed65-457f-ab4f-b9fc1f9e6b73",
   "metadata": {},
   "source": [
    "### Función de iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5011b-6e7f-4323-bfb9-11c692328d16",
   "metadata": {},
   "source": [
    "### Implementación en Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f103279-2e06-4573-bcf7-e1588fc9c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Perceptron with a hidden layer\n",
    "    '''\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.loss = None\n",
    "        self.iteration = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compil(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, epochs=1, verbose=True):\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            predicts = self.forward(X)\n",
    "            loss = self.loss(predicts, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.iteration += 1\n",
    "            if verbose:\n",
    "                print(\"iteration = %d: loss=%.3f \" % (self.iteration, loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa86fd8-7103-4dfd-9684-3f00b0f19297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,  batch_size, percentile):\n",
    "        self.percentile = percentile\n",
    "        self.batch_size = batch_size\n",
    "        self.env = None\n",
    "        self.net = None\n",
    "        self.batch = self.iterate_batches()#generator of batches\n",
    "\n",
    "    def set_net(self, net):\n",
    "        self.net = net\n",
    "    \n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    \n",
    "    def iterate_batches(self):\n",
    "        Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "        EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "        batch = []\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = []\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        obs = self.env.reset() # first observation\n",
    "        while True:\n",
    "            obs_v = torch.FloatTensor([obs])\n",
    "            act_probs_v = sm(self.net(obs_v)) # compute probabilities for next action\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            action = np.random.choice(len(act_probs), p=act_probs) # select action\n",
    "            next_obs, reward, is_done, _ = self.env.step(action) # send action to env\n",
    "            episode_reward += reward\n",
    "            step = EpisodeStep(observation=obs, action=action)\n",
    "            episode_steps.append(step)\n",
    "            if is_done:\n",
    "                e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "                batch.append(e)\n",
    "                episode_reward = 0.0\n",
    "                episode_steps = []\n",
    "                next_obs = self.env.reset()\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "            obs = next_obs\n",
    "        \n",
    "    \n",
    "    def filter_batch(self):\n",
    "        batch = next(self.batch) # generate next batch\n",
    "        rewards = list(map(lambda s: s.reward, batch)) # extract rewards from the batch\n",
    "        reward_bound = np.percentile(rewards, self.percentile) # extract top percentile reward\n",
    "        reward_mean = float(np.mean(rewards))\n",
    "\n",
    "        train_obs = []\n",
    "        train_act = []\n",
    "        for reward, steps in batch:\n",
    "            if reward < reward_bound:\n",
    "                continue\n",
    "            train_obs.extend(map(lambda step: step.observation, steps))\n",
    "            train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "        train_obs_v = torch.FloatTensor(train_obs)\n",
    "        train_act_v = torch.LongTensor(train_act)\n",
    "        return train_obs_v, train_act_v, reward_bound, reward_mean \n",
    "    \n",
    "    def get_batch(self):\n",
    "        return self.filter_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fc8553-024f-4b4c-b4bc-fd1a611b94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, env=\"CartPole-v0\"):\n",
    "        self.env = gym.make(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self.env.observation_space\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37a782a-362d-474c-99f5-614f6944e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self,  net, agent, env, reward_limit = 199):\n",
    "        self.net = net\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self. reward_limit = reward_limit\n",
    "        \n",
    "    def play(self):\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "               \n",
    "        X, y, reward_b, reward_m = self.agent.get_batch()\n",
    "        while reward_m <= self.reward_limit:\n",
    "            self.net.fit(X, y)\n",
    "            X, y, reward_b, reward_m = self.agent.get_batch()\n",
    "        \n",
    "        print('¡Resuelto!')\n",
    "        print(\"reward_mean=%.1f, rw_bound=%.1f\" % (reward_m, reward_b))    \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01088113-f096-41bb-8a95-68bf6a01e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "#\n",
    "# imports\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# env\n",
    "env = Env(env=\"CartPole-v0\")\n",
    "OBS_SIZE = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "# brain\n",
    "brain = Net(obs_size=OBS_SIZE , hidden_size=HIDDEN_SIZE, n_actions=N_ACTIONS)\n",
    "optimizer = optim.Adam(params=brain.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "brain.compil(optimizer, loss)\n",
    "\n",
    "\n",
    "\n",
    "# agent\n",
    "agent = Agent(batch_size=BATCH_SIZE, percentile=PERCENTILE)\n",
    "agent.set_net(brain)\n",
    "agent.set_env(env)\n",
    "\n",
    "u = CrossEntropy(net=brain, agent=agent, env=env, reward_limit=199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9321961f-2bf5-4aa4-98e2-37c21b5df8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 1: loss=0.683 \n",
      "iteration = 2: loss=0.665 \n",
      "iteration = 3: loss=0.652 \n",
      "iteration = 4: loss=0.649 \n",
      "iteration = 5: loss=0.623 \n",
      "iteration = 6: loss=0.600 \n",
      "iteration = 7: loss=0.608 \n",
      "iteration = 8: loss=0.586 \n",
      "iteration = 9: loss=0.589 \n",
      "iteration = 10: loss=0.575 \n",
      "iteration = 11: loss=0.551 \n",
      "iteration = 12: loss=0.562 \n",
      "iteration = 13: loss=0.548 \n",
      "iteration = 14: loss=0.577 \n",
      "iteration = 15: loss=0.542 \n",
      "iteration = 16: loss=0.518 \n",
      "iteration = 17: loss=0.551 \n",
      "iteration = 18: loss=0.512 \n",
      "iteration = 19: loss=0.531 \n",
      "iteration = 20: loss=0.524 \n",
      "iteration = 21: loss=0.527 \n",
      "iteration = 22: loss=0.532 \n",
      "iteration = 23: loss=0.532 \n",
      "iteration = 24: loss=0.527 \n",
      "iteration = 25: loss=0.512 \n",
      "iteration = 26: loss=0.512 \n",
      "iteration = 27: loss=0.526 \n",
      "iteration = 28: loss=0.524 \n",
      "¡Resuelto!\n",
      "reward_mean=200.0, rw_bound=200.0\n"
     ]
    }
   ],
   "source": [
    "u.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
